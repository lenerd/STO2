\documentclass[a4paper]{scrartcl}

% font/encoding packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[ngerman]{babel}
\usepackage[ngerman=ngerman-x-latest]{hyphsubst}

\usepackage{amsmath, amssymb, amsfonts, amsthm}
\usepackage{array}
\usepackage{stmaryrd}
\usepackage{marvosym}
\allowdisplaybreaks
\usepackage[output-decimal-marker={,}]{siunitx}
\usepackage[shortlabels]{enumitem}
\usepackage[section]{placeins}
\usepackage{float}
\usepackage{units}
\usepackage{listings}
\usepackage{pgfplots}
\pgfplotsset{compat=1.12}
\usepackage{tikz}
\usetikzlibrary{arrows,automata}

\newtheorem*{behaupt}{Behauptung}
\newcommand{\gdw}{\Leftrightarrow}
\newcommand{\N}{\mathbb{N}}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\cov}{\operatorname{Cov}}
\newcommand{\e}{\mathbb{E}}
\newcommand{\var}{\operatorname{Var}}
\newcommand{\corr}{\operatorname{Corr}}

\usepackage{fancyhdr}
\pagestyle{fancy}

\lstset{%
    frame=single,
    numbers=left,
    keepspaces,
    language=R,
    title=Listing: \lstname,
}

\def \blattnr {3}

\lhead{Stochastik 2 - Blatt {\blattnr}}
\rhead{Florian Abt, Lennart Braun, Sascha Schulz}
\cfoot{\thepage}


\title{Stochastik 2 für Informatiker}
\subtitle{Blatt {\blattnr} Hausaufgaben}
\author{
    Florian Abt (6524404), \\
    Lennart Braun (6523742), \\
    Sascha Schulz (6434677)
}
\date{zum 3. November 2015}

\begin{document}
\maketitle

\begin{enumerate}[label=\bfseries \blattnr.\arabic*]
    \item
        \begin{enumerate}[label=\alph*)]
            \item
                \begin{behaupt}
                    $x_i = 1$ für alle $i \in T$ ist eine Lösung des linearen
                    Gleichungssystems
                    \begin{equation*}
                        x_i = \sum_{c \in C} p_{ic} + \sum_{j \in T} p_{ij} x_j
                        \text{ .}
                    \end{equation*}
                \end{behaupt}
                \begin{proof}
                    Sei $x_i = 1$ für alle $i \in T$.
                    \begin{equation*}
                        \begin{split}
                            \sum_{c \in C} p_{ic} + \sum_{j \in T} p_{ij} x_j
                            &= \sum_{c \in C} p_{ic} + \sum_{j \in T} p_{ij} \\
                            &= \sum_{k \in E = C \cup T} p_{ik} \\
                            &= 1 = x_i
                        \end{split}
                    \end{equation*}
                \end{proof}

                \begin{behaupt}
                    Aus den drei Aussagen folgt die Behauptung.
                \end{behaupt}
                \begin{proof}
                    Wir wissen:
                    \begin{itemize}
                        \item
                            $x_i = \alpha_i(C)$ ist eine Lösung für das LGS.
                        \item
                            $x_i = 1$ ist eine Lösung für das LGS.
                        \item
                            Die Lösung ist eindeutig für jede Form der
                            Inhomogenität, also auch für $\gamma_i = \sum_{c
                            \in C} p_{ic}$.
                    \end{itemize}
                    Dann muss $\alpha_i(C) = x_i = 1$ für alle $i \in T$
                    gelten.
                \end{proof}

            \item
                \begin{behaupt}
                    $x_i = 1$ für $i \in \N$ ist eine Lösung für
                    \begin{equation*}
                        x_i = p_{i0} + \sum_{j=1}^\infty p_{ij} x_j
                        \text{ .}
                    \end{equation*}
                \end{behaupt}
                \begin{proof}
                    Sei $x_i = 1$ für alle $i \in \N$.
                    Zunächst betrachten wir den Fall $1 < i \in \N$.
                    \begin{equation*}
                        \begin{split}
                            p_{i0} + \sum_{j=1}^\infty p_{ij} x_j
                            &= 0 + p_{i,i-1} x_{i-1} + p_{i,i+1} x_{i+1} \\
                            &= (1-p) \cdot 1 + p \cdot 1 \\
                            &= (1-p) + p \\
                            &= 1 = x_i
                        \end{split}
                    \end{equation*}
                    Nun betrachten wir den Fall $i = 1$.
                    \begin{equation*}
                        \begin{split}
                            p_{i0} + \sum_{j=1}^\infty p_{ij} x_j
                            &= p_{1,0} + p_{1,2} x_{2} \\
                            &= (1-p)  + p \cdot 1 \\
                            &= (1-p) + p \\
                            &= 1 = x_1
                        \end{split}
                    \end{equation*}
                    $x_i = 1$ für alle $i \in \N$ ist also eine Lösung des
                    linearen Gleichungssystems.
                \end{proof}

                \begin{behaupt}
                    $x_i = \left(\frac{1-p}{p}\right)^i$ für $i \in \N$ ist
                    eine Lösung für
                    \begin{equation*}
                        x_i = p_{i0} + \sum_{j=1}^\infty p_{ij} x_j
                        \text{ .}
                    \end{equation*}
                \end{behaupt}
                \begin{proof}
                    Sei $x_i = \left(\frac{1-p}{p}\right)^i$ für alle $i \in
                    \N$.  Zunächst betrachten wir den Fall $1 < i \in \N$.
                    \begin{equation*}
                        \begin{split}
                            p_{i0} + \sum_{j=1}^\infty p_{ij} x_j
                            &= 0 + p_{i,i-1} x_{i-1} + p_{i,i+1} x_{i+1} \\
                            &= (1-p) \left( \frac{1-p}{p} \right)^{i-1} +
                                p \left( \frac{1-p}{p} \right)^{i+1} \\
                            &= \left( \frac{1-p}{p} \right)^{i-1}
                                \left( (1-p) + 
                                p \left( \frac{1-p}{p} \right)^{2} \right) \\
                            &= \left( \frac{1-p}{p} \right)^{i-1}
                                \left( (1-p) + \frac{(1-p)^2}{p} \right) \\
                            &= \left( \frac{1-p}{p} \right)^{i-1}
                                \left( \frac{p -p^2 +1 -2p +p^2}{p} \right) \\
                            &= \left( \frac{1-p}{p} \right)^{i-1}
                                \left( \frac{1-p}{p} \right) \\
                            &= \left( \frac{1-p}{p} \right)^i = x_i
                        \end{split}
                    \end{equation*}
                    Nun betrachten wir den Fall $i = 1$.
                    \begin{equation*}
                        \begin{split}
                            p_{i0} + \sum_{j=1}^\infty p_{ij} x_j
                            &= p_{1,0} + p_{1,2} x_{2} \\
                            &= (1-p) + p \cdot \left(\frac{1-p}{p}\right)^2 \\
                            &= (1-p) + \frac{(1-p)^2}{p} \\
                            &= \frac{p -p^2 +1 -2p +p^2}{p} \\
                            &= \frac{1-p}{p} \\
                            &= \left(\frac{1-p}{p}\right)^1 = x_1
                        \end{split}
                    \end{equation*}
                    $x_i = \left(\frac{1-p}{p}\right)^i$ für alle $i \in \N$
                    ist also eine Lösung des linearen Gleichungssystems.
                \end{proof}

            \item
                Wir halten $x_i = \left( \frac{1-p}{p} \right)^i$ ($\forall i
                \in \N$) für die „glaubhaftere“ Lösung.

                Die Wahrscheinlichkeit, das Spiel mit einem Startkapital von
                $a$ zu verlieren, also in den Zustand $0$ zugelangen, wurde in
                der Vorlesung mit
                \begin{equation*}
                    \alpha_a(0)
                    = 1 - \frac{1 - \left( \frac{1-p}{p} \right)^a}
                               {1 - \left( \frac{1-p}{p} \right)^{a+b}}
                \end{equation*}
                angegeben. $b$ sei hierbei das Startkapital des Gegenspielers.
                Da $\frac{1-p}{p} < 1$ gilt $\left( \frac{1-p}{p} \right)^{a+b}
                \to 0$ für $b \to \infty$.
                Für den Grenzwert der Absorbtionswahrscheinlichkeit ergibt sich
                \begin{equation*}
                    \begin{split}
                        \lim_{b \to \infty} \alpha_a(0)
                        &= 1 - \frac{1 - \left( \frac{1-p}{p} \right)^a}
                                    {1 - 0} \\
                        &= \left( \frac{1-p}{p} \right)^a = x_a
                        \text{ .}
                    \end{split}
                \end{equation*}

        \end{enumerate}

   \item
       \begin{enumerate}
           \item
               $\tau$ ist eine Zufallsvariable.

           \item
               Sei $m_i := \e[\tau\ |\ X_0 = i]$.
               \begin{behaupt}
                   Es gilt
                   \begin{equation*}
                       m_i = 1 + \sum_{j \in T} p_{ij}m_j
                       = 1 + \sum_{j \in E} p_{ij}m_j
                       , \qquad i \in T
                   \end{equation*}
                   mit $m_c = 0$ für alle $c \in C$.
               \end{behaupt}
               \begin{proof}
                   Es sei $i \in T$.
                   \begin{equation*}
                       \begin{split}
                           m_i &= \e[\tau\ |\ X_0 = i] \\
                               &\overset{(1)}{=} \sum_{j \in E}
                                    \e[\tau\ |\ X_1 = j, X_0 = i]
                                    \cdot \prob(X_1 = j\ |\ X_0 = i) \\
                               &\overset{(2)}{=} \sum_{j \in E}
                                    \e[\tau\ |\ X_1 = j]
                                    \cdot p_{ij} \\
                               &= \sum_{j \in E}
                                    \e[\tau + 1\ |\ X_0 = j]
                                    \cdot p_{ij} \\
                               &\overset{(3)}{=} \sum_{j \in E}
                                    (\e[\tau\ |\ X_0 = j] + 1)
                                    \cdot p_{ij} \\
                               &= \sum_{j \in E} (m_j + 1) \cdot p_{ij} \\
                               &= \sum_{j \in E} p_{ij} +
                                    \sum_{j \in E} p_{ij} m_j \\
                               &= 1 + \sum_{j \in E} p_{ij} m_j \\
                               &\overset{(4)}{=} 1 + \sum_{j \in T} p_{ij} m_j
                       \end{split}
                   \end{equation*}
                   Wir verwenden den Satz von der totalen Wahrscheinlichkeit
                   $(1)$, die Markov-Eigenschaft $(2)$ und die Linearität des
                   Erwartungswertes $(3)$.
                   $(4)$ gilt, da $p_{ij} = 0$ für alle $j \in C = E
                   \setminus T$.
               \end{proof}

           \item
               \begin{behaupt}
                   \begin{equation*}
                       m_i = i(a+b-i)
                   \end{equation*}
                   ist die erwartete Spieldauer bei Startkapital $i = a$ für
                   Spieler $A$, einem festen Gesamtkapital $a+b$ und $p =
                   \frac{1}{2}$ mit $m_c = 0$ für alle $c \in C$.
               \end{behaupt}
               \begin{proof}
                   Sei $0 < i < a + b$.
                   \begin{equation*}
                       \begin{split}
                           m_i &= 1 + \sum_{j \in E} p_{ij}m_j \\
                               &= 1 + p_{i,i-1}m_{i-1} + p_{i,i+1}m_{i+1} \\
                               &= 1 + \frac{1}{2} \cdot m_{i-1} + \frac{1}{2}
                                    \cdot m_{i+1} \\
                               &= 1 + \frac{1}{2} \Big( (i-1)(a+b-i+1) +
                                    (i+1)(a+b-i-1) \Big) \\
                               &= 1 + \frac{1}{2} \Big( (i-1)(a+b-i+1) +
                                    (i-1)(a+b-i-1) + 2 \cdot (a+b-i-1) \Big) \\
                               &= 1 + \frac{1}{2} \Big( (i-1)(2a+2b-2i) +
                                    2 \cdot (a+b-i-1) \Big) \\
                               &= 1 + (i-1)(a+b-i) + (a+b-i-1) \\
                               &= (i-1)(a+b-i) + (a+b-i) \\
                               &= i(a+b-i)
                       \end{split}
                   \end{equation*}
               \end{proof}
               \begin{behaupt}
                   Für eine gleichmäßige Verteilung des Startkapitals wird
                   $m_i$ maximal.
               \end{behaupt}
               \begin{proof}
                   Um ein Extremum zu finden, setzen wir die erste Ableitung von
                   $m_i$ gleich $0$.
                   \begin{align*}
                       m_i &= i(a+b-i) = i(a+b) - i^2 \\
                       \frac{\mathrm{d}}{\mathrm{d} i} m_i &= (a+b) -2i \\
                       \frac{\mathrm{d}^2}{\mathrm{d} i^2} m_i &= -2 \\
                       0 &= (a+b) -2i \gdw i = \frac{(a+b)}{2}
                   \end{align*}
                   Da die zweite Ableitung negativ ist, ist $m_i$ maximal, wenn
                   $i$ die Hälfte des Gesamtkapitals ist. Dies kann natürlich
                   nur auftreten, falls $2\ |\ (a+b)$. Andernfalls wird $m_i$
                   maximal, falls sich $|a-b|=1$.
               \end{proof}

       \end{enumerate}
%=======
   
    % \begin{enumerate}
    %  \item Varianz.
    %  \item
	% Wir wählen eine Zufallsvariable Y, welche die Anzahl der Schritte bis zum erreichen eines absorbierenden Zustands zählt.
	% 
	% Folglich ist $\mathbb{E}(\tau|X_0=i) = \mathbb{E}(Y|X_0=i)$, wobei
	% 
	% \begin{equation*}
	%   \begin{split}
	%     \mathbb{E}(Y|X_0=i) 
	% 	&= \sum_{n=1}^\infty \mathbb{P}(Y=n|X_0=i) \cdot n \\
	% 	&= 1 + \sum_{n=1}^\infty \sum_{j\in T} \mathbb{P}(Y=n, X_1=j | X_0=i) \cdot n \\
	% 	&= 1 + \sum_{j\in T} \sum_{n=1}^\infty \mathbb{P}(Y=n|X_1 = j, X_0=i) \cdot \mathbb{P}(X_1=j|X_0=i) \cdot n \\
	% 	&= 1 + \sum_{j\in T} \sum_{n=1}^\infty \mathbb{P}(Y=n|X_1 = j) \cdot p_{ij} \cdot n \\
	% 	&= 1 + \sum_{j\in T} p_{ij} \cdot \sum_{n=1}^\infty \mathbb{P}(Y=n|X_0=j) \cdot n \\
	% 	&= 1 + \sum_{j\in T} p_{ij} \cdot \mathbb{E}(Y|X_0=j)
	%   \end{split}
	% \end{equation*}
	% 
	% Wobei $\mathbb{E}(Y|X_0=j)$ dem $m_j$ entspricht.
	% 
	% Da $m_c = \mathbb{E}(Y|X_0=c) = 0$ gegeben ist, kann dies verallgemeinert werden:
	% $$ \mathbb{E}(Y|X_0=i) = 1 + \sum_{j\in T} p_{ij} \cdot \mathbb{E}(Y|X_0=j) = 1 + \sum_{j\in E} p_{ij} \cdot \mathbb{E}(Y|X_0=j)$$
	% da durch die Multiplikation mit dem Faktor 0 alle Therme für $c \in C$ ignoriert werden, welche durch die Erweiterung hinzukommen, da gerade $E\setminus T=C$.

    % \end{enumerate}

   
%>>>>>>> 3f92b32ec128c6851c0256a82707d83b1126df37

   \item 
   
     Eine invariante Verteilung setzt vorraus, dass $\pi = \pi \cdot P$,\\ wobei $\pi$ ein stochastischer Vektor ist.
   
     \begin{enumerate}
      \item
	\begin{equation*}
	 \pi = \pi \cdot P \\
	 \end{equation*}
	 \begin{equation*}
	  \begin{pmatrix}
	   p_1 & p_2 & p_3
	  \end{pmatrix}
	  =
	  \begin{pmatrix}
	   p_1 & p_2 & p_3
	  \end{pmatrix}
	  \cdot
	  \begin{pmatrix}
	    0 & \frac23 & \frac13 \\
	    \frac13 & 0 & \frac23 \\
	    \frac23 & \frac13 & 0 
	  \end{pmatrix}
	\end{equation*}

      Daraus ergibt sich das LGS:
      
      \begin{equation}
       p_1 = \frac13 p_2 + \frac23 p_3 
      \end{equation}
      \begin{equation}
       p_2 = \frac23 p_1 + \frac13 p_3 
      \end{equation}       
      \begin{equation}
	p_3 = \frac13 p_1 + \frac23 p_2
      \end{equation}       
           
      \begin{equation}
	\begin{split}
       \Rightarrow p_1 &= \frac13 p_2 + \frac23 \left(\frac13 p_1 + \frac23 p_2\right) \\
       \Rightarrow p_1 &= \frac13 p_2 + \frac29 p_1 + \frac49 p_2 \\
       \Rightarrow \frac79 p_1 &= \frac79 p_2 \\
       \Rightarrow p_1 &= p_2 \\
       \Rightarrow p_3 &= \frac13 p_1 + \frac23 p_1 \\
       \Rightarrow p_3 &= p_1
	\end{split}
      \end{equation}
      
      Folglich ist $\pi = \left(\frac13,\frac13,\frac13\right)$ die eindeutige invariante Verteilung. 
      
      Da $p_{ij} > 0$ für $i\neq j$ ist jedes Ereignis mit einem Schritt aus jedem anderen Ereignis erreichbar, somit ist
      der Ereignisraum irreduzibel.
      
      % Da zusätzlich auch $p_{ii}=0$ gilt, ist direkt ersichtlich, dass jedes Ereignis periodisch
      % mit $d_i=2$ ist, folglich ist $(X_n)_{n\in\N_0}$ periodisch.	
      Da $P^2$ nur positive Einträge enthält und $P$ eine stochastische Matrix
      ist, enhalten auch alle höheren Potenzen von $P$ ausschließlich positive
      Einträge.
      \begin{equation*}
          P^2 =
          \begin{pmatrix}
              \frac{4}{9} & \frac{1}{9} & \frac{4}{9} \\
              \frac{4}{9} & \frac{4}{9} & \frac{1}{9} \\
              \frac{1}{9} & \frac{4}{9} & \frac{4}{9} \\
          \end{pmatrix}
      \end{equation*}
      Damit gilt $p_{ii}^{(n)} > 0$ für alle $n > 1$ und alle $i \in E$. Daher
      sind alle Zustände aperiodisch.
	
      \item
	\begin{equation*}
	 \pi = \pi \cdot P \\
	 \end{equation*}
	 \begin{equation*}
	  \begin{pmatrix}
	   p_1 & p_2 & p_3
	  \end{pmatrix}
	  =
	  \begin{pmatrix}
	   p_1 & p_2 & p_3
	  \end{pmatrix}
	  \cdot
	  \begin{pmatrix}
	    \frac14 & 0 & \frac34 \\
	    0 & 1 & 0 \\
	    \frac12 & 0 & \frac12 
	  \end{pmatrix}
	\end{equation*}

      Daraus ergibt sich das Gleichungssystem:
      
      \begin{equation} \label{eq:3.3.b.I}
       p_1 = \frac14 p_1 + \frac12 p_3 \Rightarrow \frac34 p_1 = \frac12 p_3 \Rightarrow p_1 = \frac23 p_3
      \end{equation}
      \begin{equation}
       p_2 = p_2 
      \end{equation}       
      \begin{equation} \label{eq:3.3.b.III}
	p_3 = \frac34 p_1 + \frac12 p_3
      \end{equation}       

      Folglich sind alle $\pi=\left(k,c,\frac23k\right)$ mit $1 = c + \frac53k$  die invarianten Veteilungen.
      
      Irreduzibilität nicht gegeben, da der Zustand 2 absorbierend 
      und somit von ihm ausgehend kein anderer erreichbar ist.
      
      Für den Zustand 2 ist durch die Absorbtion keine Periodizität gegeben; 
      da $p_{ii}>0$ für $i \in E\setminus\{2\}$ sind auch alle anderen Zustände aperiodisch.
      
      
	
      \item
      
      \begin{equation*}
	 \pi = \pi \cdot P \\
	 \end{equation*}
	 \begin{equation*}
	  \begin{pmatrix}
	   p_1 & p_2 & p_3
	  \end{pmatrix}
	  =
	  \begin{pmatrix}
	   p_1 & p_2 & p_3
	  \end{pmatrix}
	  \cdot
	  \begin{pmatrix}
	    0 & \frac14 & \frac34 \\
	    1 & 0 & 0 \\
	    1 & 0 & 0
	  \end{pmatrix}
	\end{equation*}

      Daraus ergibt sich das Gleichungssystem:
      
      \begin{equation} \label{3.3.c.I}
       p_1 = p_2 + p_3
      \end{equation}
      \begin{equation} \label{3.3.c.II}
       p_2 = \frac14 p_1
      \end{equation}       
      \begin{equation} \label{3.3.c.III}
	p_3 = \frac34 p_1
      \end{equation}  
      
      Hieraus lässt sich die invariante Verteilung $\pi = \left(\frac12, \frac18, \frac38\right)$ ablesen, welche eindeutig ist.
      
      Der Zustandsraum $E$ ist irreduzibel, da sich jeder Zustand von jedem
      anderen aus in maximal zwei Schritten erreichen lässt. Entweder direkt
      (2 und 3 von 1 aus und umgekehrt) oder über 1 (zwischen 2 und 3).
      %% Das sehe ich irgendwie nicht.
      % Der Zustandsraum $E$ ist irreduzibel, da aus den Gleichungen (\ref{3.3.c.I}) bis (\ref{3.3.c.III}) ersichtlich wird, dass jedes Ereignis von 
      % jedem anderen Ereignis in maximal 2 Schritten erreicht werden kann.
      
      Betrachten wir die Periodizität der einzelnen Zustände:
      \begin{equation*}
          P^{2k+1} =
          \begin{pmatrix}
              0 & \frac{1}{4} & \frac{3}{4} \\
              1 & 0 & 0 \\
              1 & 0 & 0 \\
          \end{pmatrix}
          \text{ und }
          P^{2k} =
          \begin{pmatrix}
              1 & 0 & 0 \\
              0 & \frac{1}{4} & \frac{3}{4} \\
              0 & \frac{1}{4} & \frac{3}{4} \\
          \end{pmatrix}
          \qquad
          \forall k \in \N_0
      \end{equation*}
      
      Es gilt also $p^{(n)}_{ii} > 0$ mit $n \in \{ 2,4,6,\ldots \}$ und $i \in E$.
      
      Folglich haben alle Ereignisse die gleiche Periodizität $d_i=2$ und somit ist $(X_n)_{n\in\N_0}$ periodisch.
     \end{enumerate}


\end{enumerate}


\end{document}
